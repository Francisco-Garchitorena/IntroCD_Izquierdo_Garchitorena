{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Ciencia de Datos: Tarea 1\n",
    "\n",
    "Este notebook contiene el código de base para realizar la Tarea 1 del curso. Puede copiarlo en su propio repositorio y trabajar sobre el mismo.\n",
    "Las **instrucciones para ejecutar el notebook** están en la [página inicial del repositorio](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd).\n",
    "\n",
    "Se utiliza el lenguaje Python y la librería Pandas. Si no tiene ninguna familiaridad con la librería, se recomienda realizar algún tutorial introductorio (ver debajo).\n",
    "También se espera que los alumnos sean proactivos a la hora de consultar las documentaciones de las librerías y del lenguaje, para entender el código provisto.\n",
    "Además de los recursos provistos en la [página del curso](https://eva.fing.edu.uy/course/view.php?id=1378&section=1), los siguientes recursos le pueden resultar interesantes:\n",
    " - [Pandas getting started](https://pandas.pydata.org/docs/getting_started/index.html#getting-started) y [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html): Son parte de la documentación en la página oficial de Pandas.\n",
    " - [Kaggle Learn](https://www.kaggle.com/learn): Incluye tutoriales de Python y Pandas.\n",
    "\n",
    "\n",
    "Si desea utilizar el lenguaje R y está dispuesto a no utilizar (o traducir) este código de base, también puede hacerlo.\n",
    "\n",
    "En cualquier caso, **se espera que no sea necesario revisar el código para corregir la tarea**, ya que todos los resultados y análisis relevantes deberían estar en el **informe en formato PDF**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar bibliotecas (dependencias)\n",
    "Recuerde instalar los requerimientos (`requirements.txt`) en el mismo entorno donde está ejecutando este notebook (ver [README](https://github.com/DonBraulio/introCD))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame con todos los discursos:\n",
    "df_speeches = pd.read_csv('../data/us_2020_election_speeches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequear si existen NaNs\n",
    "df_speeches.isna().any().any()  # Da como reusltado True o False dependiendo si el DataFrame posee NaNs entre sus datos.\n",
    "# Localiar los NaNs dentro del DataFrame\n",
    "print(df_speeches.isna().sum())\n",
    "df_speeches[df_speeches.isna().any(axis=1)] # Filtra y devuelve solo las filas que poseen por los menos un NaN.\n",
    "\n",
    "# Analizar otras formas de datos faltantes\n",
    "sospechosos = [\"???\", \"-\", \"N/A\", \"missing\", \"\"]  # Posibles formas que podrían aparecer como datos faltantes\n",
    "df_speeches[df_speeches.isin(sospechosos).any(axis=1)] #Analizo otras formas de datos faltantes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Cargado y Limpieza de Datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analice la cantidad de discursos por candidato\n",
    "\n",
    "# Tome los 5 candidatos con más discursos\n",
    "df_speeches_top_5 = df_speeches['speaker'].value_counts().head(5)  # Analizo el número de apariciones de cada candidato en mi DataFrame y me quedo con el top 5.\n",
    "print(df_speeches_top_5)   # 5 candidatos con más discursos.\n",
    "# DataFrame con los datos de los 5 candidatos con más discursos\n",
    "df_speeches_top_5_df = df_speeches[df_speeches['speaker'].isin(df_speeches_top_5.index)] #Creo un DataFrame que posea únicamente los discursos de los 5 candidatos con más discursos.\n",
    "df_speeches_top_5_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualización de los discursos de cada candidato a lo largo del tiempo \n",
    "Meses = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Ago\", \"Sep\", \"Oct\"] #La escala temporal seleccionada es mensual. \n",
    "cantidad_disc_por_mes = {}  #Creo un diccionario que incluirá la cantidad de discursos por mes de cada candidato\n",
    "for month in Meses:\n",
    "# Quiero quedarme con la cantidad de discursos por mes en Meses\n",
    "    df_mes = df_speeches_top_5_df[df_speeches_top_5_df['date'].str.contains(month, na=False)] #checks if \"Oct\" appears in the string and ignores missing values\n",
    "    cantidad_disc_por_mes[month] = df_mes['speaker'].value_counts()\n",
    "\n",
    "\n",
    "#Generar un grafico de barras que tenga cantidad de discursos por mes para cada candidato\n",
    "df_cantidad = pd.DataFrame(cantidad_disc_por_mes)  # Creo un DataFrame para facilitar la creación del gráfico de barras.\n",
    "print(df_cantidad)  #Los NaN implican que el candidato no dio un discurso en ese mes.\n",
    "\n",
    "# Lista de candidatos\n",
    "candidatos = df_speeches_top_5_df['speaker'].unique()\n",
    "\n",
    "# Crear una paleta fija basada en Set2\n",
    "base_palette = sns.color_palette(\"Set1\", n_colors=len(candidatos))\n",
    "custom_palette = dict(zip(candidatos, base_palette))\n",
    "\n",
    "# Gráfico de barras agrupado\n",
    "ax = df_cantidad.T.plot(kind='bar', figsize=(12, 7), width=0.8, edgecolor='black', color=[custom_palette[speaker] for speaker in df_cantidad.index])\n",
    "\n",
    "# Título y etiquetas\n",
    "plt.title(\"Cantidad de discursos por mes para los principales oradores\", fontsize=22)\n",
    "plt.ylabel(\"Cantidad de discursos\", fontsize=20)\n",
    "plt.yticks(ticks=range(1, 27,3), labels=range(1, 27,3), rotation=0, fontsize=20)\n",
    "plt.xticks(rotation=0, fontsize=20)\n",
    "plt.legend(bbox_to_anchor=(0, 1), loc='upper left', fontsize=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observar la cantidad de discursos por location de cada speaker\n",
    "most_visited_locations = df_speeches_top_5_df['location'].value_counts().head(10)  #Observar la cantidad de discursos por location de cada candidato\n",
    "\n",
    "cantidad_disc_por_location = {}  #Creo un diccionario que incluirá la cantidad de discursos por mes de cada candidato\n",
    "for location in most_visited_locations.index:\n",
    "# Quiero quedarme con la cantidad de discursos por mes en Meses\n",
    "    df_location = df_speeches_top_5_df[df_speeches_top_5_df['location'].str.contains(location, na=False)] #checks if \"Oct\" appears in the string and ignores missing values\n",
    "    cantidad_disc_por_location[location] = df_location['speaker'].value_counts()\n",
    "df_cantidad_location = pd.DataFrame(cantidad_disc_por_location)\n",
    "df_cantidad_location\n",
    "#Visuallizar los datos de discurso por location de cada candidato que no sea un grafico de barras\n",
    "df_cantidad_location = df_cantidad_location.fillna(0)  # Cambia los NaN por 0 para poder graficar\n",
    "\n",
    "#Generar un grafico de torta que ilustre la proporción de discuros Virtuales por cada candidato\n",
    "location = df_cantidad_location.columns[0] #Quiero quedarme con los discursos Virtuales.\n",
    "df_cantidad_location[location].plot(kind='pie', autopct='%1.1f%%', figsize=(6,6), title=f\"Distribución de discursos en {location}\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "#Generar un grafico de torta que tenga las locaciones más frecuentes de los discursos\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(most_visited_locations, labels=most_visited_locations.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Distribución total de discursos por las 10 locations más frecuentes\")\n",
    "plt.axis('equal')  # Hace el gráfico circular\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACIÓN: PORCENTAJE DE DISCURSOS POR LOCACIÓN\n",
    "\n",
    "# Obtener los 10 locations más frecuentes\n",
    "most_visited_locations = df_speeches_top_5_df['location'].value_counts()\n",
    "print(most_visited_locations)\n",
    "# Inicializar contadores\n",
    "conteo_virtual = 0\n",
    "conteo_foxnews = 0\n",
    "conteo_presenciales = 0\n",
    "\n",
    "medios_televisivos = [\"Fox News\", \"ABC\", \"NBC\", \"CNN\"]\n",
    "\n",
    "# Hago una división en discursos Virtuales, televisivos y presenciales\n",
    "for location, count in most_visited_locations.items():\n",
    "    if \"Virtual\" in location:\n",
    "        conteo_virtual += count\n",
    "    elif any(medio in location for medio in medios_televisivos):\n",
    "        conteo_foxnews += count\n",
    "    else:\n",
    "        conteo_presenciales += count\n",
    "\n",
    "# Crear diccionario con los datos agrupados\n",
    "conteos_agrupados = {\n",
    "    \"Virtual\": conteo_virtual,\n",
    "    \"Televisivos\": conteo_foxnews,\n",
    "    \"Presenciales\": conteo_presenciales\n",
    "}\n",
    "\n",
    "colors = sns.color_palette(\"Set1\", n_colors=len(conteos_agrupados))\n",
    "# Gráfico de torta con la proporción de discursos Virtuales, Fox News y Presenciales\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(conteos_agrupados.values(), labels=conteos_agrupados.keys(), autopct='%1.1f%%', startangle=140, pctdistance=0.75, textprops={'fontsize': 18}, colors=colors, wedgeprops={'edgecolor': 'black'})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACIÓN: PORCENTAJE DE DISCURSOS POR LOCACIÓN ANTES Y DESPUES DE JULIO\n",
    "\n",
    "# Asegurar que la columna 'date' es de tipo datetime\n",
    "df_speeches_top_5_df['date'] = pd.to_datetime(df_speeches_top_5_df['date'])\n",
    "\n",
    "# Definir fecha de corte: 1 de julio\n",
    "fecha_corte = pd.to_datetime('Jul 1, 2020')  # Cambiá el año si es otro\n",
    "\n",
    "# Separar el DataFrame\n",
    "df_antes_julio = df_speeches_top_5_df[df_speeches_top_5_df['date'] < fecha_corte]\n",
    "df_despues_julio = df_speeches_top_5_df[df_speeches_top_5_df['date'] >= fecha_corte]\n",
    "\n",
    "# Función para agrupar discursos por tipo de location\n",
    "def contar_tipos(df):\n",
    "    conteo_virtual = 0\n",
    "    conteo_foxnews = 0\n",
    "    conteo_presenciales = 0\n",
    "    medios_televisivos = [\"Fox News\", \"ABC\", \"NBC\", \"CNN\"]\n",
    "    \n",
    "    for location, count in df['location'].value_counts().items():\n",
    "        if \"Virtual\" in location:\n",
    "            conteo_virtual += count\n",
    "        elif any(medio in location for medio in medios_televisivos):\n",
    "            conteo_foxnews += count\n",
    "        else:\n",
    "            conteo_presenciales += count\n",
    "    return {\n",
    "        \"Virtual\": conteo_virtual,\n",
    "        \"Televisivos\": conteo_foxnews,\n",
    "        \"Presenciales\": conteo_presenciales\n",
    "    }\n",
    "\n",
    "# Obtener conteos para ambos periodos\n",
    "conteos_antes = contar_tipos(df_antes_julio)\n",
    "conteos_despues = contar_tipos(df_despues_julio)\n",
    "\n",
    "# Graficar los dos gráficos de torta lado a lado\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].pie(conteos_antes.values(), labels=conteos_antes.keys(), autopct='%1.1f%%', startangle=140, textprops={'fontsize': 20}, colors=colors, wedgeprops={'edgecolor': 'black'})\n",
    "axs[0].set_title('Antes de julio', fontsize=20)\n",
    "\n",
    "axs[1].pie(conteos_despues.values(), labels=conteos_despues.keys(), autopct='%1.1f%%', startangle=140, textprops={'fontsize': 20}, colors=colors, wedgeprops={'edgecolor': 'black'})\n",
    "axs[1].set_title('Desde julio', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "df_despues_julio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Texto y Conteo de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En los discursos se observa la presencia de diálogos entre los candidatos y agentes externos (entevistadores, multitudes, etc.) Dado que únicamente interesa lo que \n",
    "# efectivamente dice el candidato, se eliminará todo lo no dicho por él, así como todas las estampas de tiempo que correspondientes a las transcripciones.\n",
    "\n",
    "#Dentro de los discursos, y en sus transcripciones, a los candidatos se los menciona de distintas formas. Nos interesa quedarnos con todo lo que dice el candidato, sea como sea \n",
    "# que se lo nombre. \n",
    "aliases = {\n",
    "    \"Donald Trump\": [\"Donald Trump\", \"President Trump\", \"Mr. Trump\", \"President Donald J. Trump\"],\n",
    "    \"Joe Biden\": [\"Joe Biden\", \"President Biden\", \"Mr. Biden\"],\n",
    "    \"Mike Pence\": [\"Mike Pence\", \"Vice President Pence\", \"Mr. Pence\"],\n",
    "    \"Kamala Harris\": [\"Kamala Harris\", \"Senator Harris\", \"Mrs. Harris\"],\n",
    "    \"Bernie Sanders\": [\"Bernie Sanders\", \"Senator Sanders\", \"Mr. Sanders\"]\n",
    "}\n",
    "\n",
    "def extract_candidate_speech(text, candidate):\n",
    "    valid_aliases = aliases.get(candidate, [])\n",
    "\n",
    "    # Captura encabezados como \"Nombre del candidato: (HH:MM:SS)\" o \"(MM:SS)\"\n",
    "    pattern = re.compile(r\"^(.*?):\\s*\\((\\d{1,2}:)?\\d{1,2}:\\d{2}\\)\\s*\\n\", re.MULTILINE)\n",
    "\n",
    "    matches = list(pattern.finditer(text))  # busco todas las coincidencias de pattern en el texto. finditer analiza el texto buscando las coincidencias y devuelve la posición en el texto donde se encuentra el pattern.\n",
    "    speeches = []\n",
    "\n",
    "    for i, match in enumerate(matches):  #Itero en todas las coincidencias\n",
    "        \n",
    "        speaker = match.group(1).strip() # Devuelve la persona que habla (candidato o audiencia o cualquier otra persona que habló.)\n",
    "        \n",
    "        start = match.end() #Devuelve el índice del primer caracter posterior al pattern. \n",
    "        \n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text) #Devuelve el índice del primer caracter previo al sigueinte pattern.\n",
    "        \n",
    "        speech_text = text[start:end].strip()  #Se queda con el texto de los discursos (todo lo posterior a los encabezados)\n",
    "        \n",
    "        if speaker in valid_aliases: # A partir de aqui me quedo solo con el texto de los candidatos.\n",
    "            # Elimina etiquetas como [inaudible 00:06:17], [crosstalk 01:02:33], etc.\n",
    "            clean_speech = re.sub(r\"\\[[^\\]]*\\d{1,2}:\\d{2}(?::\\d{2})?\\]\", \"\", speech_text) # Quito ciertas estructuras con la función re.sub()\n",
    "            \n",
    "            speeches.append(clean_speech.strip())\n",
    "\n",
    "    return \"\\n\".join(speeches) # Devuelvo las partes del discurso dichas por el candidato, separadas por párrafos (\\n).\n",
    "\n",
    "# Aplicamos la limpieza de partes del discurso que no nos interesan. La aplicación se hace sobre cada fila del DataFrame.\n",
    "df_speeches_top_5_df[\"pre_processed_text\"] = df_speeches_top_5_df.apply(\n",
    "    lambda row: extract_candidate_speech(row[\"text\"], row[\"speaker\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Guardamos el resultado en un csv para luego analizar cómo se generó el clean_text.\n",
    "# df_speeches_top_5_df[\"pre_processed_text\"].to_csv(\"pre_processed_text.csv\", index=False) \n",
    "\n",
    "# Analizo que haya funcionado correctamente.\n",
    "print(df_speeches_top_5_df.loc[10,\"pre_processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza del texto: Interesa quitar todos los signos de puntuación, pasar el texto a minúsculas y eliminar contracciones.\n",
    "\n",
    "def expand_contractions(text):   # Función para eliminar contracciones, basado en una lista de contracciones y sus respectivas palabras completas.\n",
    "    contractions = {\n",
    "        r\"\\bcan’t\\b\": \"cannot\",   #con el r indico que es una cadena cruda, por ende el \\b me separa la palabra can't de otras palabras que la contengan.\n",
    "        r\"\\bwon’t\\b\": \"will not\",\n",
    "        r\"\\bit’s\\b\": \"it is\",\n",
    "        r\"\\bi’m\\b\": \"i am\",\n",
    "        r\"\\bi’ve\\b\": \"i have\",\n",
    "        r\"\\bi’d\\b\": \"i would\",\n",
    "        r\"\\bi’ll\\b\": \"i will\",\n",
    "        r\"\\byou’re\\b\": \"you are\",\n",
    "        r\"\\byou’ve\\b\": \"you have\",\n",
    "        r\"\\byou’d\\b\": \"you would\",\n",
    "        r\"\\byou’ll\\b\": \"you will\",\n",
    "        r\"\\bhe’s\\b\": \"he is\",\n",
    "        r\"\\bshe’s\\b\": \"she is\",\n",
    "        r\"\\bthey’re\\b\": \"they are\",\n",
    "        r\"\\bwe’re\\b\": \"we are\",\n",
    "        r\"\\bdon’t\\b\": \"do not\",\n",
    "        r\"\\bdidn’t\\b\": \"did not\",\n",
    "        r\"\\bdoesn’t\\b\": \"does not\",\n",
    "        r\"\\bwasn’t\\b\": \"was not\",\n",
    "        r\"\\bweren’t\\b\": \"were not\",\n",
    "        r\"\\bcouldn’t\\b\": \"could not\",\n",
    "        r\"\\bshouldn’t\\b\": \"should not\",\n",
    "        r\"\\bwouldn’t\\b\": \"would not\",\n",
    "        r\"\\bthat’s\\b\": \"that is\",\n",
    "        r\"\\bthere’s\\b\": \"there is\",\n",
    "        r\"\\bwhat’s\\b\": \"what is\",\n",
    "        r\"\\bwho’s\\b\": \"who is\",\n",
    "        r\"\\blet’s\\b\": \"let us\",\n",
    "        r\"\\bisn’t\\b\": \"is not\",\n",
    "        r\"\\baren’t\\b\": \"are not\",\n",
    "        r\"\\bgonna\\b\": \"going to\",\n",
    "        r\"\\bwanna\\b\": \"want to\",\n",
    "        r\"\\bgotta\\b\": \"got to\"\n",
    "    }\n",
    "    for pattern, repl in contractions.items():\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def clean_text(df, column_name):\n",
    "    \n",
    "    # Eliminar primeras palabras hasta el primer \"\\n\"\n",
    "    result = df[column_name].str.replace(r\"^[^\\n]*\\n\", \"\", regex=True)\n",
    "    \n",
    "    # Convertir todo a minúsculas\n",
    "    result = result.str.lower()\n",
    "    \n",
    "    # Expandir contracciones\n",
    "    result = result.apply(expand_contractions)\n",
    "\n",
    "    # Reemplazar signos de puntuación faltantes\n",
    "    for punc in [\"[\", \"]\", \"\\n\", \",\", \":\", \"?\", \"(\", \")\", \"!\", \".\", \"...\",\";\", \"¿\", \"¡\", \"“\", \"”\", '\"', \"'\", \"‘\", \"’\", \"{\", \"}\", \"%\",\"$\",\"0\",\"1\",\"-\",\"/\"]: # agregue: \"%\" \"(\", \")\", \"!\", \".\", \";\", \"¿\", \"¡\", \"“\", \"”\", '\"', \"'\", \"‘\", \"’\", \"{\", \"}\"\n",
    "        result = result.str.replace(punc, \" \")\n",
    "\n",
    "    # Reemplazar saltos de línea por espacios (por si quedaron después del paso 1)\n",
    "    result = result.str.replace(r\"\\n\", \" \", regex=True)\n",
    "\n",
    "    # Eliminar múltiples espacios con uno solo (normaliza frases)\n",
    "    result = result.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # Eliminar espacios al inicio o final\n",
    "    result = result.str.strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crear una nueva columna CleanText a partir de 'text'\n",
    "df_speeches_top_5_df.loc[:, \"CleanText\"] = clean_text(df_speeches_top_5_df, \"pre_processed_text\")\n",
    "df_speeches_top_5_df.loc[1, \"CleanText\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Conteo de Palabras y Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Candidatos con mayor cantidad de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------Obtener la cantidad de palabras por discurso---------------------------------------------------------------# \n",
    "\n",
    "# Convierte párrafos en listas \"palabra1 palabra2 palabra3\" -> [\"palabra1\", \"palabra2\", \"palabra3\"]\n",
    "df_speeches_top_5_df[\"WordList\"] = df_speeches_top_5_df.loc[:,\"CleanText\"].str.split()\n",
    "\n",
    "# contar palabras en la columna wordlist para cada discurso\n",
    "df_speeches_top_5_df.loc[:,\"WordCount\"] = df_speeches_top_5_df[\"WordList\"].str.len()\n",
    "\n",
    "# ver cantidad de palabras por discurso\n",
    "df_speeches_top_5_df[[\"CleanText\", \"WordList\", \"WordCount\"]]\n",
    "\n",
    "#--------------------------------Obtener la cantidad de veces que se menciona cada palabra dicha por cada candidato-------------------------# \n",
    "\n",
    "# Explota la columna WordList\n",
    "df_exploded = df_speeches_top_5_df[[\"speaker\", \"WordList\"]].explode(\"WordList\")\n",
    "# Agrupa y cuenta cuántas veces se usa cada palabra por speaker\n",
    "grouped_wordlist = df_exploded.groupby([\"speaker\", \"WordList\"]).size().reset_index(name=\"count\") # obtengo un DataFrame que tiene la cantidad de veces que cada palabra fue utilizada por cada candidato (columna count).\n",
    "print(grouped_wordlist) #Obtengo un DataFrame que tiene la cantidad de veces que cada palabra fue utilizada por cada candidato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALISIS DE LAS PALABRAS MÁS EXPRESADAS POR CADA CANDIDATO\n",
    "\n",
    "df_candidates_words = pd.DataFrame()  # Inicializamos como DataFrame vacío\n",
    "\n",
    "# Recorremos cada speaker y seleccionamos sus top 5 palabras más frecuentes\n",
    "for speaker in grouped_wordlist[\"speaker\"].unique():\n",
    "    top_words = (\n",
    "        grouped_wordlist[grouped_wordlist[\"speaker\"] == speaker] # Filtramos por el speaker actual\n",
    "        .sort_values(\"count\", ascending=False) # Ordenamos por cantidad de veces que se repite la palabra\n",
    "        .head(5) # Seleccionamos las 5 palabras más frecuentes\n",
    "        .reset_index(drop=True) # Reseteamos el índice para evitar problemas al concatenar\n",
    "    )\n",
    "    top_words[\"speaker\"] = speaker  # Agregamos columna con el nombre del speaker\n",
    "    df_candidates_words = pd.concat([df_candidates_words, top_words], ignore_index=True)\n",
    "\n",
    "df_candidates_words # DataFrame con el top 5 palabras de cada candidato\n",
    "\n",
    "# Gráfico de barras para las palabras más frecuentes por candidato\t\n",
    "\n",
    "# las palabras pueden repetirse entre candidatos, pero cada barra debe reflejar cuántas veces la dijo ese candidato. Para que eso funcione y las barras no se apilen, necesitamos \n",
    "# que el eje x distinga cada ocurrencia de una palabra por cada candidato, incluso si la palabra se repite.\n",
    "#La solución: mantenemos solo la palabra en el eje x (para mostrarla), pero usamos el índice del DataFrame como orden, de forma que cada barra sea única.\n",
    "\n",
    "# Graficar con el índice como eje x para que no se colapsen\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=df_candidates_words,\n",
    "    x=df_candidates_words.index,         # .index hace que cada barra sea única osea dos palabras iguales si tienen distinto index se grafican independientes\n",
    "    y='count',\n",
    "    hue='speaker',\n",
    "    dodge=False,\n",
    "    edgecolor='black',\n",
    "    palette=custom_palette  \n",
    ")\n",
    "\n",
    "# Reemplazar los valores del eje x por las palabras\n",
    "plt.xticks(ticks=df_candidates_words.index, labels=df_candidates_words['WordList'], rotation=45, ha='right')\n",
    "plt.xlabel(\"Palabra\", fontsize=20)\n",
    "plt.xticks(rotation = 45,fontsize=15)\n",
    "plt.ylabel(\"Cantidad de veces expresada\", fontsize=20)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"Top 5 palabras más frecuentes por candidato\" , fontsize=22)\n",
    "#plt.grid(axis='y', linestyle='--', alpha=0.7)  # Añadir líneas de cuadrícula en el eje y\n",
    "plt.legend(title=\"Candidato\", fontsize=12, title_fontsize=13, loc='upper left', bbox_to_anchor=(0, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_candidates_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALISIS DEL TOTAL DE PALABRAS HABLADAS POR CADA CANDIDATO\n",
    "\n",
    "total_words_by_speaker = grouped_wordlist.groupby(\"speaker\")[\"count\"].sum().to_dict()\n",
    "\n",
    "# Añadir columna con frecuencia relativa\n",
    "df_candidates_words[\"relative_freq\"] = df_candidates_words.apply(\n",
    "    lambda row: row[\"count\"] / total_words_by_speaker[row[\"speaker\"]],\n",
    "    axis=1\n",
    ")\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(\n",
    "    data=df_candidates_words,\n",
    "    x=df_candidates_words.index,\n",
    "    y=\"relative_freq\",\n",
    "    hue=\"speaker\",\n",
    "    dodge=False,\n",
    "    edgecolor=\"black\",\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "plt.xticks(ticks=df_candidates_words.index, labels=df_candidates_words['WordList'], rotation=30, ha='right')\n",
    "plt.xlabel(\"Palabra\", fontsize=20)\n",
    "plt.ylabel(\"Frecuencia relativa\", fontsize=20)\n",
    "plt.title(\"Top 5 palabras más frecuentes por candidato (normalizado)\", fontsize=22)\n",
    "plt.xticks(rotation=30, fontsize=15, ha='right')\n",
    "# Dibujar una rayita pequeña bajo cada barra\n",
    "for x in df_candidates_words.index:\n",
    "    plt.plot([x, x], [0, 0.001], color='black', linewidth=1, clip_on=False)\n",
    "plt.xlim(-0.48, 24.5)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(title=\"Candidato\", fontsize=14, title_fontsize=15, loc='upper left', bbox_to_anchor=(0.028, 1.015))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Busque los candidatos/as con mayor cantidad de palabras.\n",
    "\n",
    "# sumar wordcount por speaker / esto nos da la cantidad de palabras totales por candidato\n",
    "totalWords_df = df_speeches_top_5_df.groupby('speaker')['WordCount'].sum().reset_index(name='TotalWords').sort_values(by='TotalWords', ascending=False)\n",
    "                                                                \n",
    "\n",
    "\n",
    "# Graficar la cantidad de palabras totales por candidato\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=totalWords_df,\n",
    "    x=totalWords_df['speaker'],\n",
    "    y=totalWords_df['TotalWords'],\n",
    "    hue='speaker',  # Añadido para distinguir los candidatos       \n",
    "    dodge=False,\n",
    "    edgecolor='black',\n",
    "    palette=custom_palette  \n",
    ")\n",
    "# Quiero que los colores se respeten del gráfico anterior, por lo que los defino manualmente\n",
    "colors = sns.color_palette(\"Set2\", n_colors=len(totalWords_df['speaker'].unique()))\n",
    "\n",
    "plt.xticks(ticks=totalWords_df['speaker'], rotation=0, ha='center', fontsize=20)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"\", fontsize=20)\n",
    "plt.ylabel(\"Cantidad de palabras totales\", fontsize=20)\n",
    "plt.title(\"Palabras totales por candidato\", fontsize=22)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estilo más formal\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(\n",
    "    data=totalWords_df,\n",
    "    x='TotalWords',\n",
    "    y='speaker',\n",
    "    hue = 'speaker',\n",
    "    palette= custom_palette,\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "\n",
    "# Agregar valores al final de las barras\n",
    "for i, (value, name) in enumerate(zip(totalWords_df[\"TotalWords\"], totalWords_df[\"speaker\"])):\n",
    "    plt.text(value + 3000, i, f\"{value:,}\", va='center', fontsize=13, color='black')\n",
    "\n",
    "plt.title(\"Cantidad total de palabras por candidato\", weight='bold')\n",
    "plt.xlabel(r\"Total de palabras ($\\times 10^3$)\")\n",
    "xticks_values = plt.xticks()[0]  # Obtener los valores actuales de los xticks\n",
    "plt.xticks(ticks=xticks_values, labels=[f\"{int(x/1e3)}\" for x in xticks_values])\n",
    "\n",
    "# plt.xticks(fontsize = 17)\n",
    "plt.ylabel(\"Candidato\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ANALIZA LA CANTIDAD DE PALABRAS ÚNICAS POR CANDIDATO\n",
    "# Agrupar por speaker y unir todas las palabras de todos los discursos\n",
    "unique_words_per_speaker = (\n",
    "    df_speeches_top_5_df\n",
    "    .groupby('speaker')['WordList']   #esta línea agrupa todas las palabras de todos los discursos por speaker\n",
    "    .apply(lambda lists: len(set(word for sublist in lists for word in sublist)))  # Con las palabras en una única lista, aplico el set para elminiar duplicados y cuento la cantidad de palabras únicas que hay.\n",
    "    .reset_index(name='UniqueWords')\n",
    "    .sort_values(by='UniqueWords', ascending=False)\n",
    ")\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(\n",
    "    data=unique_words_per_speaker,\n",
    "    x='speaker',\n",
    "    y='UniqueWords',\n",
    "    hue='speaker',\n",
    "    dodge=False,\n",
    "    edgecolor='black',\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "\n",
    "plt.xticks(ticks=unique_words_per_speaker['speaker'],rotation=0, ha='center', fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"\", fontsize=20)\n",
    "plt.ylabel(\"Cantidad real de palabras distintas\", fontsize=20)\n",
    "plt.title(\"Cantidad real de palabras distintas por candidato\", fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graficar\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=unique_words_per_speaker,\n",
    "    x='UniqueWords',\n",
    "    y='speaker',\n",
    "    hue='speaker',\n",
    "    dodge=False,\n",
    "    edgecolor='black',\n",
    "    palette=custom_palette\n",
    ")\n",
    "sns.despine(left=True, bottom=True, right=True, top=True)\n",
    "\n",
    "# Agregar valores al final de las barras\n",
    "for i, (value, name) in enumerate(zip(unique_words_per_speaker[\"UniqueWords\"], unique_words_per_speaker[\"speaker\"])):\n",
    "    plt.text(value + 100, i, f\"{value:,}\", va='center', fontsize=13, color='black')\n",
    "\n",
    "# plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"\", fontsize=20)\n",
    "plt.ylabel(\"Candidato\")\n",
    "plt.title(\"Cantidad de palabras distintas por candidato\", weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la matriz de menciones entre candidatos.\n",
    "# Primero normalizamos las formas de mención de los candidatos utlizando un set de alias por candidato. \n",
    "# Diccionario de sinónimos\n",
    "candidate_aliases = {\n",
    "    \"joe biden\": [\"vice president biden\", \"vicepresident biden\", \"senator biden\", \"biden\", \"joe biden\"],\n",
    "    \"donald trump\": [\"president trump\", \"trump\", \"donald trump\"],\n",
    "    \"mike pence\": [\"mike pence\", \"vice president pence\"],\n",
    "    \"bernie sanders\": [\"bernie sanders\", \"senator sanders\", \"mr sanders\"],\n",
    "    \"kamala harris\": [\"kamala harris\", \"senator harris\", \"mrs harris\"]\n",
    "}\n",
    "\n",
    "def normalize_candidate_mentions(text, alias_dict):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    for canonical_name, aliases in alias_dict.items():\n",
    "        # Ordenar de mayor a menor longitud para evitar que \"joe\" reemplace antes que \"joe biden\"\n",
    "        sorted_aliases = sorted(aliases, key=len, reverse=True)\n",
    "        for alias in sorted_aliases:\n",
    "            pattern = fr\"\\b{re.escape(alias)}\\b\"\n",
    "            text = re.sub(pattern, canonical_name, text, flags=re.IGNORECASE) # busco el alias y lo sustituyo por el nombre del candidato\n",
    "    return text\n",
    "\n",
    "\n",
    "# Aplicamos la normalización\n",
    "df_speeches_top_5_df[\"CleanText_normalized\"] = df_speeches_top_5_df[\"CleanText\"].apply(\n",
    "    lambda t: normalize_candidate_mentions(t, candidate_aliases)\n",
    ")\n",
    "# Obtener la lista de speakers únicos en minúsculas\n",
    "speakers = df_speeches_top_5_df[\"speaker\"].str.lower().unique()\n",
    "mentions_matrix = pd.DataFrame(0, index=speakers, columns=candidate_aliases.keys())\n",
    "\n",
    "# Contar menciones reales usando CleanText_normalized\n",
    "for candidate in candidate_aliases.keys():\n",
    "    for speaker in speakers:\n",
    "        discursos_speaker = df_speeches_top_5_df[df_speeches_top_5_df[\"speaker\"].str.lower() == speaker]\n",
    "        total_mentions = discursos_speaker[\"CleanText_normalized\"].str.count(fr\"\\b{candidate}\\b\").sum()\n",
    "        mentions_matrix.loc[speaker, candidate] = total_mentions\n",
    "mentions_matrix\n",
    "\n",
    "#Visualizacion\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(mentions_matrix, annot=True, cmap=\"Reds\", fmt=\"d\")\n",
    "plt.title(\"Menciones entre candidatos\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
